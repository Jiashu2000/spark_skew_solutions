{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3997a181",
   "metadata": {},
   "source": [
    "## Experiment on IMDB Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6377943a",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "60c7d485",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import DataFrame\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c860703",
   "metadata": {},
   "source": [
    "### Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e92265e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf2_setup():\n",
    "    conf = SparkConf() \\\n",
    "        .set('spark.driver.memory', '20G') \\\n",
    "        .set('spark.sql.shuffle.partitions', '16') \\\n",
    "        .set('spark.sql.adaptive.enabled', 'false') \\\n",
    "        \n",
    "    spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local[8]')\\\n",
    "        .config(conf = conf)\\\n",
    "        .appName(\"conf2\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    url = spark.sparkContext.uiWebUrl\n",
    "    print(\"Spark web UI: \", url)\n",
    "    \n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a70a817",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf3_setup():\n",
    "    conf = SparkConf() \\\n",
    "        .set('spark.driver.memory', '20G') \\\n",
    "        .set('spark.sql.shuffle.partitions', '16') \\\n",
    "        .set('spark.sql.autoBroadcastJoinThreshold', '-1') \\\n",
    "        .set('spark.sql.adaptive.enabled', 'true') \\\n",
    "        .set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        \n",
    "    spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local[8]')\\\n",
    "        .config(conf = conf)\\\n",
    "        .appName(\"conf3\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    url = spark.sparkContext.uiWebUrl\n",
    "    print(\"Spark web UI: \", url)\n",
    "    \n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f607e641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf5_setup():\n",
    "    conf = SparkConf() \\\n",
    "        .set('spark.driver.memory', '20G') \\\n",
    "        .set('spark.sql.shuffle.partitions', '16') \\\n",
    "        .set('spark.sql.autoBroadcastJoinThreshold', '-1') \\\n",
    "        .set('spark.sql.adaptive.enabled', 'true') \\\n",
    "        .set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\") \\\n",
    "        .set(\"spark.sql.adaptive.skewJoin.skewedPartitionFactor\", \"3\")\n",
    "        \n",
    "    spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master('local[8]')\\\n",
    "        .config(conf = conf)\\\n",
    "        .appName(\"conf5\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    url = spark.sparkContext.uiWebUrl\n",
    "    print(\"Spark web UI: \", url)\n",
    "    \n",
    "    return spark_session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7feddd7e",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "086c640a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf2_query9():\n",
    "    # set up spark session\n",
    "    spark = conf2_setup()\n",
    "\n",
    "    # read data\n",
    "    principals = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/title.principals.tsv\")\n",
    "    \n",
    "    names = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/name.basics.tsv\")\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    top4000 = principals\\\n",
    "    .groupBy('nconst')\\\n",
    "    .agg(F.count(F.lit(1)).alias(\"num_rows\")).sort(F.col(\"num_rows\").desc())\\\n",
    "    .select('nconst')\\\n",
    "    .limit(4000)\n",
    "    \n",
    "    name_to_change = top4000.rdd.map(lambda x: x[0]).collect()\n",
    "    \n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    principals = principals.withColumn('nconst',\n",
    "                   F.when(F.col('nconst').isin(name_to_change), F.lit(\"nm0914844\"))\n",
    "                   .otherwise(F.col('nconst')))\n",
    "    \n",
    "    \"\"\"\n",
    "    # check data skewness \n",
    "    principals\\\n",
    "        .groupBy('nconst')\\\n",
    "        .agg(F.count(F.lit(1)).alias(\"num_rows\")).sort(F.col(\"num_rows\").desc())\\\n",
    "        .show(truncate=False, n = 30)\n",
    "    \"\"\"\n",
    "    \n",
    "    # join two tables and group by\n",
    "    principals.join(names, principals['nconst'] == names['nconst'], 'inner')\\\n",
    "        .groupBy(\"birthYear\") \\\n",
    "        .agg(F.avg(F.length('primaryName')).alias(\"avg_name_length\")) \\\n",
    "        .show(truncate = False, n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e375d670",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf2_query10():\n",
    "    # set up spark session\n",
    "    spark = conf2_setup()\n",
    "\n",
    "    # read data\n",
    "    principals = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/title.principals.tsv\")\n",
    "    \n",
    "    names = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/name.basics.tsv\")\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    top4000 = principals\\\n",
    "    .groupBy('nconst')\\\n",
    "    .agg(F.count(F.lit(1)).alias(\"num_rows\")).sort(F.col(\"num_rows\").desc())\\\n",
    "    .select('nconst')\\\n",
    "    .limit(4000)\n",
    "    \n",
    "    name_to_change = top4000.rdd.map(lambda x: x[0]).collect()\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    principals = principals.withColumn('nconst',\n",
    "                   F.when(F.col('nconst').isin(name_to_change), F.lit(\"nm0914844\"))\n",
    "                   .otherwise(F.col('nconst')))\n",
    "    # key salting\n",
    "    salting_n = 5\n",
    "\n",
    "    principals = principals\\\n",
    "        .withColumn(\"salt_key\", F.floor(F.rand(222)*(salting_n-1)))\n",
    "\n",
    "    names = names \\\n",
    "        .withColumn('key2', F.array([F.lit(num) for num in range(0, salting_n)]))\\\n",
    "        .withColumn('key2', F.explode(F.col('key2')))\n",
    "    \n",
    "    # join two tables and group by\n",
    "    principals.join(names, (principals['nconst'] == names['nconst']) & (principals['salt_key'] == names['key2']), \n",
    "                           'inner')\\\n",
    "        .groupBy(\"birthYear\") \\\n",
    "        .agg(F.avg(F.length('primaryName')).alias(\"avg_name_length\")) \\\n",
    "        .show(truncate = False, n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b7d5e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf3_query11():\n",
    "    # set up spark session\n",
    "    spark = conf3_setup()\n",
    "\n",
    "    # read data\n",
    "    principals = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/title.principals.tsv\")\n",
    "    \n",
    "    names = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/name.basics.tsv\")\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    top4000 = principals\\\n",
    "    .groupBy('nconst')\\\n",
    "    .agg(F.count(F.lit(1)).alias(\"num_rows\")).sort(F.col(\"num_rows\").desc())\\\n",
    "    .select('nconst')\\\n",
    "    .limit(4000)\n",
    "    \n",
    "    name_to_change = top4000.rdd.map(lambda x: x[0]).collect()\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    principals = principals.withColumn('nconst',\n",
    "                   F.when(F.col('nconst').isin(name_to_change), F.lit(\"nm0914844\"))\n",
    "                   .otherwise(F.col('nconst')))\n",
    "    \n",
    "    # join two tables and group by\n",
    "    principals.join(names, principals['nconst'] == names['nconst'], 'inner')\\\n",
    "        .groupBy(\"birthYear\") \\\n",
    "        .agg(F.avg(F.length('primaryName')).alias(\"avg_name_length\")) \\\n",
    "        .show(truncate = False, n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b460f9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conf5_query12():\n",
    "    # set up spark session\n",
    "    spark = conf5_setup()\n",
    "\n",
    "    # read data\n",
    "    principals = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/title.principals.tsv\")\n",
    "    \n",
    "    names = spark.read.format(\"csv\")\\\n",
    "        .option(\"header\", \"true\")\\\n",
    "        .option(\"delimiter\", \"\\t\")\\\n",
    "        .option('inferSchema','true')\\\n",
    "        .load(\"file:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/name.basics.tsv\")\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    top4000 = principals\\\n",
    "    .groupBy('nconst')\\\n",
    "    .agg(F.count(F.lit(1)).alias(\"num_rows\")).sort(F.col(\"num_rows\").desc())\\\n",
    "    .select('nconst')\\\n",
    "    .limit(4000)\n",
    "    \n",
    "    name_to_change = top4000.rdd.map(lambda x: x[0]).collect()\n",
    "    \n",
    "    # manipulate data to make it more skew\n",
    "    principals = principals.withColumn('nconst',\n",
    "                   F.when(F.col('nconst').isin(name_to_change), F.lit(\"nm0914844\"))\n",
    "                   .otherwise(F.col('nconst')))\n",
    "    \n",
    "    # join two tables and group by\n",
    "    principals.join(names, principals['nconst'] == names['nconst'], 'inner')\\\n",
    "        .groupBy(\"birthYear\") \\\n",
    "        .agg(F.avg(F.length('primaryName')).alias(\"avg_name_length\")) \\\n",
    "        .show(truncate = False, n = 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e901e14",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# conf2_query9()\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# conf2_query10()\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43mconf3_query11\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# conf5_query12()\u001b[39;00m\n\u001b[1;32m     10\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mconf3_query11\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconf3_query11\u001b[39m():\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# set up spark session\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     spark \u001b[38;5;241m=\u001b[39m \u001b[43mconf3_setup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# read data\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     principals \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mread\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mheader\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;241m.\u001b[39moption(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minferSchema\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile:///Users/jiashu/Documents/UW_CSE544/project/experiments_finals/data/data2/title.principals.tsv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36mconf3_setup\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconf3_setup\u001b[39m():\n\u001b[0;32m----> 2\u001b[0m     conf \u001b[38;5;241m=\u001b[39m \u001b[43mSparkConf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \\\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.driver.memory\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m20G\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.sql.shuffle.partitions\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m16\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.sql.autoBroadcastJoinThreshold\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-1\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.sql.adaptive.enabled\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m'\u001b[39m) \\\n\u001b[1;32m      7\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.coalescePartitions.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;241m.\u001b[39mset(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspark.sql.adaptive.skewJoin.enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     10\u001b[0m     spark_session \u001b[38;5;241m=\u001b[39m SparkSession\\\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;241m.\u001b[39mbuilder\\\n\u001b[1;32m     12\u001b[0m         \u001b[38;5;241m.\u001b[39mmaster(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlocal[8]\u001b[39m\u001b[38;5;124m'\u001b[39m)\\\n\u001b[1;32m     13\u001b[0m         \u001b[38;5;241m.\u001b[39mconfig(conf \u001b[38;5;241m=\u001b[39m conf)\\\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconf3\u001b[39m\u001b[38;5;124m\"\u001b[39m) \\\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;241m.\u001b[39mgetOrCreate()\n\u001b[1;32m     17\u001b[0m     url \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39muiWebUrl\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/pyspark/conf.py:132\u001b[0m, in \u001b[0;36mSparkConf.__init__\u001b[0;34m(self, loadDefaults, _jvm, _jconf)\u001b[0m\n\u001b[1;32m    128\u001b[0m _jvm \u001b[38;5;241m=\u001b[39m _jvm \u001b[38;5;129;01mor\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_jvm\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# JVM is created, so create self._jconf directly through JVM\u001b[39;00m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jconf \u001b[38;5;241m=\u001b[39m \u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSparkConf\u001b[49m(loadDefaults)\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;66;03m# JVM is not created, so store data in self._conf first\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1712\u001b[0m, in \u001b[0;36mJVMView.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1709\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;241m==\u001b[39m UserHelpAutoCompletion\u001b[38;5;241m.\u001b[39mKEY:\n\u001b[1;32m   1710\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[0;32m-> 1712\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\n\u001b[1;32m   1715\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1716\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer \u001b[38;5;241m==\u001b[39m proto\u001b[38;5;241m.\u001b[39mSUCCESS_PACKAGE:\n\u001b[1;32m   1717\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gateway_client, jvm_id\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_id)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/python/lib/py4j-0.10.9.7-src.zip/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # conf2_query9()\n",
    "    # conf2_query10()\n",
    "    \n",
    "    conf3_query11()\n",
    "    # conf5_query12()\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Elapsed_time: {end_time - start_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b72eba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
